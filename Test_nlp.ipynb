{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Text Generation with Neural Networks"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions for Processing Text\n",
    "\n",
    "### Reading in files as a string text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_file(filepath):\n",
    "    \n",
    "    with open(filepath) as f:\n",
    "        str_text = f.read()\n",
    "    \n",
    "    return str_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = read_file('moby_dick_four_chapters.txt')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenize and Clean Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'all_headlines' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 9\u001b[0m\n\u001b[1;32m      6\u001b[0m     txt \u001b[39m=\u001b[39m txt\u001b[39m.\u001b[39mencode(\u001b[39m\"\u001b[39m\u001b[39mutf8\u001b[39m\u001b[39m\"\u001b[39m)\u001b[39m.\u001b[39mdecode(\u001b[39m\"\u001b[39m\u001b[39mascii\u001b[39m\u001b[39m\"\u001b[39m,\u001b[39m'\u001b[39m\u001b[39mignore\u001b[39m\u001b[39m'\u001b[39m) \n\u001b[1;32m      7\u001b[0m     \u001b[39mreturn\u001b[39;00m txt\n\u001b[0;32m----> 9\u001b[0m corpus \u001b[39m=\u001b[39m [clean_text(sentence) \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m all_headlines] \n\u001b[1;32m     10\u001b[0m \u001b[39mprint\u001b[39m(corpus[:\u001b[39m10\u001b[39m])\n\u001b[1;32m     12\u001b[0m nlp \u001b[39m=\u001b[39m spacy\u001b[39m.\u001b[39mload(\u001b[39m\"\u001b[39m\u001b[39men_core_web_sm\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'all_headlines' is not defined"
     ]
    }
   ],
   "source": [
    "# Nettoyage de données\n",
    "import spacy\n",
    "\n",
    "def clean_text(txt): \n",
    "    txt = \"\".join(t for t in txt if t not in string.punctuation).lower() \n",
    "    txt = txt.encode(\"utf8\").decode(\"ascii\",'ignore') \n",
    "    return txt\n",
    "\n",
    "corpus = [clean_text(sentence) for x in all_headlines] \n",
    "print(corpus[:10])\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "# Create a blank Tokenizer with just the English vocab\n",
    "tokenizer = Tokenizer(nlp.vocab)\n",
    "tokenizer = Tokenizer() def get_sequence_of_tokens(corpus): ## tokenisation tokenizer.fit_on_texts(corpus) total_words = len(tokenizer.word_index) + 1 ## convertir les données en une séquence de jetons input_sequences = [] pour la ligne dans le corpus : token_list = tokenizer .texts_to_sequences([line])[0] for i in range(1, len(token_list)): n_gram_sequence = token_list[:i+1] input_sequences.append(n_gram_sequence) return input_sequences, total_words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Sequences of Tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keras"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Keras Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# integer encode sequences of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert to Numpy Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating an LSTM based model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense,LSTM,Embedding, Bidirectional,Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'n_unique_words' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m model \u001b[39m=\u001b[39m Sequential() \n\u001b[0;32m----> 2\u001b[0m model\u001b[39m.\u001b[39madd(Embedding(n_unique_words, \u001b[39m128\u001b[39m, input_length\u001b[39m=\u001b[39mmaxlen)) \n\u001b[1;32m      3\u001b[0m model\u001b[39m.\u001b[39madd(Bidirectional(LSTM(\u001b[39m64\u001b[39m))) \n\u001b[1;32m      4\u001b[0m model\u001b[39m.\u001b[39madd(Dropout(\u001b[39m0.5\u001b[39m)) \n",
      "\u001b[0;31mNameError\u001b[0m: name 'n_unique_words' is not defined"
     ]
    }
   ],
   "source": [
    "model = Sequential() \n",
    "model.add(Embedding(n_unique_words, 128, input_length=maxlen)) \n",
    "model.add(Bidirectional(LSTM(64))) \n",
    "model.add(Dropout(0.5)) \n",
    "model.add(Dense(1, activation ='sigmoid')) \n",
    "model.compile(loss='binary_crossentropy', optimiseur='adam', metrics=['accuracy'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train / Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history=model.fit(x_train, y_train, batch_size=batch_size, epochs=12, validation_data=[x_test, y_test]) \n",
    "print(history.history['loss']) \n",
    "print(history.history['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing import sequence\n",
    "max_len = 200 \n",
    "x_train = sequence.pad_sequences(x_train, maxlen=maxlen) \n",
    "x_test = sequence.pad_sequences(x_test, maxlen=maxlen) \n",
    "y_test = np.array(y_test) \n",
    "y_train = np.array(y_train)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history=model.fit(x_train, y_train, batch_size=batch_size, epochs=12, validation_data=[x_test, y_test]) \n",
    "print(history.history['loss']) \n",
    "print(history.history['accuracy'])\n",
    "\n",
    "from matplotlib import pyplot \n",
    "pyplot.plot(history.history['loss']) \n",
    "pyplot.plot(history.history['accuracy']) \n",
    "pyplot.title('model loss vs precision') \n",
    "pyplot.xlabel('epoch') \n",
    "pyplot .legend(['loss', 'accuracy'], loc='upper right') \n",
    "pyplot.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating New Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import randint\n",
    "from pickle import load\n",
    "from keras.models import load_model\n",
    "from keras.utils import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Great Job!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
